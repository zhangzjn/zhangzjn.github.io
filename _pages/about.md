---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>


I am <strong>Jiangning Zhang (å¼ æ±Ÿå®)</strong>, currently working as a expert researcher of two centers at YouTu Lab, Tencent, Shanghai (æŠ€æœ¯å¤§å’–è®¡åˆ’), also serve as the leader for [APRIL-AIGC](https://april.zju.edu.cn/our-team) direction. I receive Ph.D. degree in College of Control Science and Engineering, Zhejiang University, Hangzhou, China, under the supervision of [Prof. Yong Liu](https://april.zju.edu.cn/our-team). My research interests include:<br>
ğŸŒ± <span style="color:#b02418; font-weight:bold;"><strong>Multimodal Perception and Generation</strong></span>: ğŸ‹ Image/Video Generation/Editing ğŸ‹ Long-term Efficient Video Understanding/RAG ğŸ‹ Agent Applications ğŸ‹ Interdisciplinary Fundamental Researches.<br>
ğŸŒ± <span style="color:#b02418; font-weight:bold;"><strong>Efficient Learning</strong></span>: ğŸ¥ Neural Architecture Design ğŸ¥ Light-weight (Ultra-High Definition) Image/Video Model ğŸ¥ Efficient Training/Finetuning.<br>
ğŸŒ± <span style="color:#b02418; font-weight:bold;"><strong>Embodied AI</strong></span>: ğŸ‰ World Model ğŸ‰ Sim2Real2Sim ğŸ‰ VLA with Humanoid/Quadruped/Wheeled Robots ğŸ‰ Action Transfer

ğŸ”¥ğŸ”¥ğŸ”¥ <span style="color:#b02418; font-weight:bold;"><strong>æˆ‘æ­£åœ¨å¯»æ‰¾å…·å¤‡è‡ªé©±åŠ›çš„2027çº§ç¡•å£«ç ”ç©¶ç”Ÿã€ç›´åšç”Ÿã€åšå£«ç”Ÿï¼Œä»¥åŠç§‘ç ”å®ä¹ ç”Ÿã€ç§‘ç ”åŠ©ç†ä¸åšå£«ååŠ å…¥[åˆ˜å‹‡æ•™æˆ](https://april.zju.edu.cn/our-team)è¯¾é¢˜ç»„ï¼Œæœ‰ä¸Šè¿°ç ”ç©¶æ–¹å‘ç›¸å…³ç§‘ç ”ç»å†è€…ä¼˜å…ˆè€ƒè™‘ã€‚</strong></span> ğŸ”¥ğŸ”¥ğŸ”¥<br>
ğŸ”¥ğŸ”¥ğŸ”¥ <span style="color:#b02418; font-weight:bold;"><strong>I am looking for self-motivated prospective master students, direct-entry PhD students, PhD students (2027 intake), as well as research interns, research assistants and postdoctoral fellows to join [Prof. Yong Liu's](https://april.zju.edu.cn/our-team) research group. Candidates with research experience in the aforementioned research areas will be given priority.</strong></span> ğŸ”¥ğŸ”¥ğŸ”¥<br>

# ğŸ”¥ News
<div style="max-height: 36em; overflow-y: auto;">
  <ol style="list-style-type: none;">
  <li> ğŸ”¥ğŸ”¥ğŸ”¥ Checkout our recent <a href="https://zhangzjn.github.io/projects/T3-Video/">T3-Video (Transform Trained Transformer)</a> for Accelerating Naive 4K Video Generation. </li>
  <li> ğŸ”¥ğŸ”¥ğŸ”¥ Checkout our recent <a href="https://zhangzjn.github.io/projects/Soul/">Soul</a> for High-fidelity Long-term Multimodal Animation. </li>
  <li> ğŸ”¥ğŸ”¥ğŸ”¥ Checkout our recent <a href="https://lewandofskee.github.io/projects/OpenVE/">OpenVE-3M</a> for High-Quality Instruction-Guided Video Editing. </li>
  <!-- <li> ğŸ”¥ğŸ”¥ğŸ”¥ Checkout our recent <a href="https://ryanchenyn.github.io/projects/IVEBench/">IVEBench</a> for Instruction-Guided Video Editing Assessment. </li> -->
    <li><i>2026.01.27</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://ryanchenyn.github.io/projects/IVEBench/">IVEBench</a>, <a href="https://juntaojianggavin.github.io/projects/M3CoTBench/">M3CoTBench</a>, <a href="https://github.com/BigAandSmallq/SAD">SAD</a>, <a href="https://github.com/Yuan-Hou/Human-MME">Human-MME</a>, and <a href="https://github.com/YU-deep/ViF">ViF</a> are accepted by <strong>ICLR 2026</strong>. </li>
    <!-- <li> ğŸ”¥ğŸ”¥ğŸ”¥ Checkout our recent <a href="https://xzc-zju.github.io/projects/UltraVideo/">UltraVideo</a> and <a href="https://huggingface.co/APRIL-AIGC/UltraWan">UltraWan</a> for UHD 4K video generation. </li> -->
    <!-- <li> ğŸ”¥ğŸ”¥ğŸ”¥ Checkout our recent <a href="https://arxiv.org/abs/2506.13589">AdaVideoRAG</a> for long video understanding. </li> -->
    <li><i>2025.12.08</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ Our team obtains the grand championğŸ¥‡ğŸ¥‡ğŸ¥‡ of the offline competition in <a href="https://www.atecup.com/competitions/atec2025">ATEC 2025 â€¢ AI & Robotics Real-World Challenges</a> <strong>(US$150,000 prize)</strong>. </li>
    <li><i>2025.11.08</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://sjtuplayer.github.io/projects/UltraGen/">UltraGen</a> and <a href="https://github.com/SassyRong/AdaKD/">AdaKD (Oral)</a> are accepted by <strong>AAAI 2026</strong>. </li>
    <!-- <li> ğŸ”¥ğŸ”¥ğŸ”¥ Checkout our recent <a href="https://github.com/zhangzjn/EMOv2/">EMOv2</a> and <a href="https://github.com/lewandofskee/MobileMamba/">MobileMamba</a> for mobile applications. </li> -->
    <li><i>2025.09.21</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://ieeexplore.ieee.org/document/11197930">ImitDiff</a> is accepted by <strong>RA-L 2025</strong>. </li>
    <li><i>2025.09.19</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://arxiv.org/abs/2506.13589">AdaVideoRAG</a> and <a href="https://xzc-zju.github.io/projects/UltraVideo/">UltraVideo</a> are accepted by <strong>NeurIPS 2025</strong>. </li>
    <li><i>2025.08.08</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://github.com/lewandofskee/MVAD">MVAD</a> is accepted by <strong>TMM 2025</strong>. </li>
    <li><i>2025.07.26</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://github.com/zhangzjn/EMOv2/">EMOv2</a> is accepted by <strong>T-PAMI 2025</strong>. </li>
    <li><i>2025.07.12</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://arxiv.org/abs/2406.02263">M3DM-NR</a> is accepted by <strong>T-PAMI 2025</strong>. </li>
    <li><i>2025.07.11</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://github.com/zhangzjn/ADer">ADer</a>, <a href="https://arxiv.org/abs/2507.11003">FiSeCLIP</a>, and <a href="https://arxiv.org/abs/2403.06403">PointSeg</a> are accepted by <strong>ICCVW 2025</strong>. </li>
    <li><i>2025.07.06</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://arxiv.org">StrandDesigner</a> is accepted by <strong>ACM MM 2025</strong>. </li>
    <li><i>2025.06.26</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://github.com/Fantasyele/LLaVA-KD">LLaVA-KD</a>, <a href="https://github.com/zhouyiks/CoLVA">CoLVA</a>, and <a href="https://shi-qingyu.github.io/DeT.github.io/">DeT</a> are accepted by <strong>ICCV 2025</strong>. </li>
    <li><i>2025.06.05</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://www.sciencedirect.com/science/article/abs/pii/S0031320325004340">MMoFusion</a> is accepted by <strong>PR 2025</strong>. </li>
    <li><i>2025.02.27</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://github.com/sjtuplayer/IAR">IAR</a>, <a href="https://github.com/lewandofskee/MobileMamba">MobileMamba</a>, <a href="https://github.com/GroundingFace/GroundingFace">GroundingFace</a>, <a href="https://aigc-explorer.github.io/TIMotion-page/">TIMotion</a>, <a href="https://lingjiekong-fdu.github.io/">AnyMaker</a>, <a href="https://realiad4ad.github.io/Real-IAD/">Real-IAD DÂ³</a>, <a href="https://wangzhiyaoo.github.io/SVFR/">SVFR</a>, <a href="https://arxiv.org/abs/2409.11367">OSV</a>, <a href="https://pengchengpcx.github.io/EditFT/">EditFT</a>, and <a href="https://jixiaozhong.github.io/Sonic/">Sonic</a> are accepted by <strong>CVPR 2025</strong>. </li>
    <!-- , <a href="https://papercopilot.com/paper-list/cvpr-paper-list/cvpr-2025-paper-list/">ranked 14th globally</a>. </li> -->
    <li><i>2025.01.29</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://zhangzjn.github.io/projects/ViTAD/">ViTAD</a> is accepted by <strong>CVIU 2025</strong>. </li>
    <li><i>2025.01.22</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://sjtuplayer.github.io/projects/SaRA/">SaRA</a> is accepted by <strong>ICLR 2025</strong>. </li>
    <li><i>2024.12.10</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://arxiv.org/abs/2405.15214">PointRWKV</a>, <a href="https://arxiv.org/abs/2406.16710">	
ID-Sculpt</a>, <a href="https://arxiv.org/abs/2403.00762">PCM</a>, and <a href="https://arxiv.org/abs/2403.09616">RefLDMSeg</a> are accepted by <strong>AAAI 2025</strong>. </li>
    <li><i>2024.09.26</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://github.com/jianzongwu/MotionBooth">MotionBooth</a>, <a href="">Fetch-and-Forge</a>, and <a href="https://lewandofskee.github.io/projects/MambaAD/">MambaAD</a> are accepted by <strong>NeurIPS 2024</strong>. </li>
    <li><i>2024.07.16</i>: &nbsp;ğŸ”¥ğŸ”¥ğŸ”¥ Checkout our recent <a href="https://sjtuplayer.github.io/projects/MotionMaster">MotionMaster</a>, a training-free camera-motion transferred video generation method. </li>
    <li><i>2024.07.16</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://sjtuplayer.github.io/projects/MotionMaster">MotionMaster</a>, <a href="https://fcchit.github.io/mambagesture/">MambaGesture</a>, and <a href="https://xiaofenmao.github.io/web-project/MDT-A2G/">MDT-A2G</a> are accepted by <strong>ACM MM 2024</strong>. </li>
    <li><i>2024.07.01</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://arxiv.org/abs/2401.03145">LSFA</a>, <a href="https://arxiv.org/abs/2405.15763">FreeMotion</a>, <a href="">AdaCLIP</a>, <a href="https://ggxxii.github.io/texdreamer">TexDreamer (Oral)</a>, <a href="https://faceadapter.github.io/face-adapter.github.io">Face-Adapter</a>, and <a href="https://arxiv.org/abs/2403.06168">DiffuMatting</a> are accepted by <strong>ECCV 2024</strong>. </li>
    <!-- , <a href="https://papercopilot.com/paper-list/eccv-paper-list/eccv-2024-paper-list/">ranked 72th globally</a>. </li> -->
    <li><i>2024.06.16</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://github.com/hanyue1648/RefT">ReferenceTwice</a> for few-shot IS is accepted by <strong>T-PAMI 2024</strong>. </li>
    <li><i>2024.06.01</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://github.com/zhangzjn/GPT-4V-AD">GPT-4V-AD</a> and <a href="https://arxiv.org/abs/2311.00453">CLIP-AD</a> are accepted by <strong>IJCAI 2024</strong>. </li>
    <li><i>2024.04.17</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://github.com/hithqd/UniM-OV3D">UniM-OV3D</a> is accepted by <strong>IJCAI 2024</strong>. </li>
    <li><i>2024.04.17</i>: &nbsp;ğŸ”¥ğŸ”¥ğŸ”¥ We release a visual Anomaly Detection toolbox <a href="https://github.com/zhangzjn/ader">ADer</a>. </li>
    <li><i>2024.03.20</i>: &nbsp;ğŸ”¥ğŸ”¥ğŸ”¥ We release the largest industrial anomaly detection dataset <a href="https://realiad4ad.github.io/Real-IAD">Real-IAD</a>. </li>
    <li><i>2024.02.27</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ <a href="https://realiad4ad.github.io/Real-IAD">Real-IAD</a>, <a href="https://portraitbooth.github.io">PortraitBooth</a>, <a href="https://github.com/sjtuplayer/SuperSVG">SuperSVG</a>, and <a href="https://jianzongwu.github.io/projects/rovi">ROVI</a> are accepted by <strong>CVPR 2024</strong>. </li>
    <li><i>2024.02.12</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ Strong backbone <a href="https://github.com/zhangzjn/EATFormer">EATFormer</a> is accepted by <strong>IJCV 2024</strong>. </li>
    <li><i>2024.01.30</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>T-PAMI 2024</strong>. </li>
    <li><i>2023.12.14</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>ICASSP 2024</strong>. </li>
    <li><i>2023.12.09</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ Four papers are accepted by <strong>AAAI 2024</strong>. </li>
    <li><i>2023.07.26</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ Two papers are accepted by <strong>ACM MM 2023</strong>. </li>
    <li><i>2023.07.14</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ Four papers are accepted by <strong>ICCV 2023</strong>. </li>
    <li><i>2023.07.08</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>IJCV 2023</strong>. </li>
    <li><i>2023.07.07</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>TIP 2023</strong>. </li>
    <li><i>2023.05.13</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ 1st place for <a href="https://codalab.lisn.upsaclay.fr/competitions/12499#results">Zero-shot Track</a> and 4th place for <a href="https://codalab.lisn.upsaclay.fr/competitions/12500#results">Few-shot Track</a> in <a href="https://sites.google.com/view/vand-cvpr23/home">Visual Anomaly and Novelty Detection (VAND) 2023 Challenge</a> by <strong>CVPR 2023</strong>. </li>
    <li><i>2023.03.09</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ Six papers are accepted by <strong>CVPR 2023</strong>. </li>
    <li><i>2022.11.22</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>AAAI 2023</strong>. </li>
    <li><i>2022.11.11</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>PRL 2022</strong>. </li>
    <li><i>2022.10.11</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ Happy graduation! ğŸ‰ğŸ‰ğŸ‰ Working in Tencent Youtu Lab, Shanghai. ğŸ”­ğŸ”­ğŸ”­ </li>
    <li><i>2022.10.01</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>ACM TOG 2022</strong>. </li>
    <li><i>2022.08.17</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>TMM</strong>. </li>
    <li><i>2022.07.09</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ Three papers are accepted by <strong>ECCV 2022</strong>. </li>
    <li><i>2022.06.18</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>TCSVT</strong>. </li>
    <li><i>2022.06.07</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One of five finalists for the <strong>IJIRA Best Paper Award 2022</strong>. </li>
    <li><i>2022.04.21</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>IJCAI 2022</strong>. </li>
    <li><i>2022.03.03</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>CVPR 2022</strong>. </li>
    <li><i>2021.12.01</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>AAAI 2022</strong>. </li>
    <li><i>2021.09.29</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>NeurIPS 2021</strong>. </li>
    <li><i>2021.09.27</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is presented in <strong>SPL 2021</strong>. </li>
    <li><i>2021.08.12</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is presented in <strong>TNNLS 2021</strong>. </li>
    <li><i>2021.07.23</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is presented in <strong>ICCV 2021</strong>. </li>
    <li><i>2020.12.12</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ Research intern in YouTu Lab, Tencent, mentored by Researcher <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=xiK4nFUAAAAJ">Yabiao Wang</a> and <a href="https://tyshiwo.github.io">Dr. Ying Tai</a>. </li>
    <li><i>2020.07.03</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>ECCV 2020 as spotlight presentation</strong>. </li>
    <li><i>2020.04.01</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ Pursuing Ph.D. in Zhejiang University, under the supervision of <a href="https://april.zju.edu.cn/our-team">Prof. Yong Liu</a>. ğŸ”­ğŸ”­ğŸ”­ </li>
    <li><i>2020.02.27</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ Two papers are accepted by <strong>CVPR 2020</strong>. </li>
    <li><i>2020.01.25</i>: &nbsp;ğŸ‰ğŸ‰ğŸ‰ One paper is accepted by <strong>ICASSP 2020</strong>. </li>
  </ol>
</div>

