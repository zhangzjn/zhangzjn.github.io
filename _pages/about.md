---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>


<!--
### Hi there 👋
**zhangzjn/zhangzjn** is a ✨ _special_ ✨ repository because its `README.md` (this file) appears on your GitHub profile.

Here are some ideas to get you started:

- 🔭 I’m currently working on ...
- 🌱 I’m currently learning ...
- 👯 I’m looking to collaborate on ...
- 🤔 I’m looking for help with ...
- 💬 Ask me about ...
- 📫 How to reach me: ...
- 😄 Pronouns: ...
- ⚡ Fun fact: ...

https://www.emojiall.com/zh-hans/all-cate
-->


I am <strong>Jiangning Zhang (张江宁)</strong>, currently working as a Principal Researcher of two teams (Industry Perception and AIGC) at YouTu Lab, Tencent, Shanghai (技术大咖计划). I receive Ph.D. degree in College of Control Science and Engineering, Zhejiang University, Hangzhou, China, under the supervision of [Prof. Yong Liu](https://april.zju.edu.cn/our-team). My research interests include:<br>
🌱 Neural Architecture Design: transformer-based architecture, light-wight vision model.<br>
🌱 Multi-modal GAN-/VAE-/Diffusion-based AIGC researches: image/video generation, multi-modal human-centric editing and generation, and 2D/3D virtual digital human related researches (3D face/body/hand/cloth reconstruction, multi-modal digital human animation, motion generation, etc.).<br>
🌱 Controllable Video Generation, RAG for LLM, Efficient Training. 

💬 Feel free to drop me emails (186368@zju.edu.cn) if you have interests on above topics, and remote cooperations are welcomed.<br>
💬 You can contact me if you are applying for a Research Intern or a B.S./Ph.D. student in computer vision / robotic perception, and I co-supervise students with [Prof. Yong Liu](https://april.zju.edu.cn/our-team) at Zhejiang University.<br>
💖 I love 📷photography, 🍲cooking, and 🌏traveling, enjoy together!!!!!! <br>
🔥🔥🔥 <span style="color:#b02418; font-weight:bold;"><strong>我在协助[刘勇教授](https://april.zju.edu.cn/our-team)招科研助理（RA）以及2026年硕士/博士生，(M)LLM Agent和多模态视频生成方向方向，有相关方向科研经历优先，欢迎本科实习生，感兴趣的同学欢迎邮件联系我。</strong></span> 🔥🔥🔥<br>
🔥🔥🔥 <span style="color:#b02418; font-weight:bold;"><strong>I am assisting [Prof. Yong Liu](https://april.zju.edu.cn/our-team) in recruiting Research Assistants (RAs) and master's / doctoral students for 2026. The focus areas include (M)LLM Agent and multimodal video generation. Candidates with relevant research experience are preferred. Undergraduate interns are also welcome. Interested individuals are encouraged to contact me via email.</strong></span> 🔥🔥🔥<br>
<!-- I (and my interns) had at most 8 disposable high-memory GPUs in all my previous studies. I would like to thank external collaborators who provided the computing resources. Long live eating grass and producing milk! -->
📝 [Publications](https://zhangzjn.github.io/#-publications)

<!-- My research interest includes neural machine translation and computer vision. I have published more than 100 papers at the top international AI conferences with total <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'>google scholar citations <strong><span id='total_cit'>260000+</span></strong></a> (You can also use google scholar badge <a href='https://scholar.google.com/citations?user=DhtAFkwAAAAJ'><img src="https://img.shields.io/endpoint?url={{ url | url_encode }}&logo=Google%20Scholar&labelColor=f6f6f6&color=9cf&style=flat&label=citations"></a>). -->


# 🔥 News
<div style="max-height: 36em; overflow-y: auto;">
  <ol style="list-style-type: none;">
    <li><i></i> 🔥🔥🔥 Checkout our recent <a href="https://github.com/zhangzjn/EMOv2/">EMOv2</a> and <a href="https://github.com/lewandofskee/MobileMamba/">MobileMamba</a> for mobile applications. </li>
    <li><i>2025.02.27</i>: &nbsp;🎉🎉🎉 <a href="https://github.com/sjtuplayer/IAR">IAR</a>, <a href="https://github.com/lewandofskee/MobileMamba">MobileMamba</a>, <a href="https://github.com/GroundingFace/GroundingFace">GroundingFace</a>, <a href="https://aigc-explorer.github.io/TIMotion-page/">TIMotion</a>, <a href="https://lingjiekong-fdu.github.io/">AnyMaker</a>, <a href="https://realiad4ad.github.io/Real-IAD/">Real-IAD D³</a>, <a href="https://wangzhiyaoo.github.io/SVFR/">SVFR</a>, <a href="https://arxiv.org/abs/2409.11367">OSV</a>, <a href="https://pengchengpcx.github.io/EditFT/">EditFT</a>, and <a href="https://jixiaozhong.github.io/Sonic/">Sonic</a> are accepted by <strong>CVPR 2025</strong>, <a href="https://papercopilot.com/paper-list/cvpr-paper-list/cvpr-2025-paper-list/">ranked 14th globally</a>. </li>
    <li><i>2025.01.29</i>: &nbsp;🎉🎉🎉 <a href="https://zhangzjn.github.io/projects/ViTAD/">ViTAD</a> is accepted by <strong>CVIU 2025</strong>. </li>
    <li><i>2025.01.22</i>: &nbsp;🎉🎉🎉 <a href="https://sjtuplayer.github.io/projects/SaRA/">SaRA</a> is accepted by <strong>ICLR 2025</strong>. </li>
    <li><i>2024.12.10</i>: &nbsp;🎉🎉🎉 <a href="https://arxiv.org/abs/2405.15214">PointRWKV</a>, <a href="https://arxiv.org/abs/2406.16710">	
ID-Sculpt</a>, <a href="https://arxiv.org/abs/2403.00762">PCM</a>, and <a href="https://arxiv.org/abs/2403.09616">RefLDMSeg</a> are accepted by <strong>AAAI 2025</strong>. </li>
    <li><i>2024.09.26</i>: &nbsp;🎉🎉🎉 <a href="https://github.com/jianzongwu/MotionBooth">MotionBooth</a>, <a href="">Fetch-and-Forge</a>, and <a href="https://lewandofskee.github.io/projects/MambaAD/">MambaAD</a> are accepted by <strong>NeurIPS 2024</strong>. </li>
    <li><i>2024.07.16</i>: &nbsp;🔥🔥🔥 Checkout our recent <a href="https://sjtuplayer.github.io/projects/MotionMaster">MotionMaster</a>, a training-free camera-motion transferred video generation method. </li>
    <li><i>2024.07.16</i>: &nbsp;🎉🎉🎉 <a href="https://sjtuplayer.github.io/projects/MotionMaster">MotionMaster</a>, <a href="https://fcchit.github.io/mambagesture/">MambaGesture</a>, and <a href="https://xiaofenmao.github.io/web-project/MDT-A2G/">MDT-A2G</a> are accepted by <strong>ACM MM 2024</strong>. </li>
    <li><i>2024.07.01</i>: &nbsp;🎉🎉🎉 <a href="https://arxiv.org/abs/2401.03145">LSFA</a>, <a href="https://arxiv.org/abs/2405.15763">FreeMotion</a>, <a href="">AdaCLIP</a>, <a href="https://ggxxii.github.io/texdreamer">TexDreamer (Oral)</a>, <a href="https://faceadapter.github.io/face-adapter.github.io">Face-Adapter</a>, and <a href="https://arxiv.org/abs/2403.06168">DiffuMatting</a> are accepted by <strong>ECCV 2024</strong>, <a href="https://papercopilot.com/paper-list/eccv-paper-list/eccv-2024-paper-list/">ranked 72th globally</a>. </li>
    <li><i>2024.06.16</i>: &nbsp;🎉🎉🎉 <a href="https://github.com/hanyue1648/RefT">ReferenceTwice</a> for few-shot IS is accepted by <strong>T-PAMI 2024</strong>. </li>
    <li><i>2024.06.01</i>: &nbsp;🎉🎉🎉 <a href="https://github.com/zhangzjn/GPT-4V-AD">GPT-4V-AD</a> and <a href="https://arxiv.org/abs/2311.00453">CLIP-AD</a> are accepted by <strong>IJCAI 2024</strong>. </li>
    <li><i>2024.04.17</i>: &nbsp;🎉🎉🎉 <a href="https://github.com/hithqd/UniM-OV3D">UniM-OV3D</a> is accepted by <strong>IJCAI 2024</strong>. </li>
    <li><i>2024.04.17</i>: &nbsp;🔥🔥🔥 We release a visual Anomaly Detection toolbox <a href="https://github.com/zhangzjn/ader">ADer</a>. </li>
    <li><i>2024.03.20</i>: &nbsp;🔥🔥🔥 We release the largest industrial anomaly detection dataset <a href="https://realiad4ad.github.io/Real-IAD">Real-IAD</a>. </li>
    <li><i>2024.02.27</i>: &nbsp;🎉🎉🎉 <a href="https://realiad4ad.github.io/Real-IAD">Real-IAD</a>, <a href="https://portraitbooth.github.io">PortraitBooth</a>, <a href="https://github.com/sjtuplayer/SuperSVG">SuperSVG</a>, and <a href="https://jianzongwu.github.io/projects/rovi">ROVI</a> are accepted by <strong>CVPR 2024</strong>. </li>
    <li><i>2024.02.12</i>: &nbsp;🎉🎉🎉 Strong backbone <a href="https://github.com/zhangzjn/EATFormer">EATFormer</a> is accepted by <strong>IJCV 2024</strong>. </li>
    <li><i>2024.01.30</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>T-PAMI 2024</strong>. </li>
    <li><i>2023.12.14</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>ICASSP 2024</strong>. </li>
    <li><i>2023.12.09</i>: &nbsp;🎉🎉🎉 Four papers are accepted by <strong>AAAI 2024</strong>. </li>
    <li><i>2023.07.26</i>: &nbsp;🎉🎉🎉 Two papers are accepted by <strong>ACM MM 2023</strong>. </li>
    <li><i>2023.07.14</i>: &nbsp;🎉🎉🎉 Four papers are accepted by <strong>ICCV 2023</strong>. </li>
    <li><i>2023.07.08</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>IJCV 2023</strong>. </li>
    <li><i>2023.07.07</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>TIP 2023</strong>. </li>
    <li><i>2023.05.13</i>: &nbsp;🎉🎉🎉 1st place for <a href="https://codalab.lisn.upsaclay.fr/competitions/12499#results">Zero-shot Track</a> and 4th place for <a href="https://codalab.lisn.upsaclay.fr/competitions/12500#results">Few-shot Track</a> in <a href="https://sites.google.com/view/vand-cvpr23/home">Visual Anomaly and Novelty Detection (VAND) 2023 Challenge</a> by <strong>CVPR 2023</strong>. </li>
    <li><i>2023.03.09</i>: &nbsp;🎉🎉🎉 Six papers are accepted by <strong>CVPR 2023</strong>. </li>
    <li><i>2022.11.22</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>AAAI 2023</strong>. </li>
    <li><i>2022.11.11</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>PRL 2022</strong>. </li>
    <li><i>2022.10.11</i>: &nbsp;🎉🎉🎉 Happy graduation! 🎉🎉🎉 Working in Tencent Youtu Lab, Shanghai. 🔭🔭🔭 </li>
    <li><i>2022.10.01</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>ACM TOG 2022</strong>. </li>
    <li><i>2022.08.17</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>TMM</strong>. </li>
    <li><i>2022.07.09</i>: &nbsp;🎉🎉🎉 Three papers are accepted by <strong>ECCV 2022</strong>. </li>
    <li><i>2022.06.18</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>TCSVT</strong>. </li>
    <li><i>2022.06.07</i>: &nbsp;🎉🎉🎉 One of five finalists for the <strong>IJIRA Best Paper Award 2022</strong>. </li>
    <li><i>2022.04.21</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>IJCAI 2022</strong>. </li>
    <li><i>2022.03.03</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>CVPR 2022</strong>. </li>
    <li><i>2021.12.01</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>AAAI 2022</strong>. </li>
    <li><i>2021.09.29</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>NeurIPS 2021</strong>. </li>
    <li><i>2021.09.27</i>: &nbsp;🎉🎉🎉 One paper is presented in <strong>SPL 2021</strong>. </li>
    <li><i>2021.08.12</i>: &nbsp;🎉🎉🎉 One paper is presented in <strong>TNNLS 2021</strong>. </li>
    <li><i>2021.07.23</i>: &nbsp;🎉🎉🎉 One paper is presented in <strong>ICCV 2021</strong>. </li>
    <li><i>2020.12.12</i>: &nbsp;🎉🎉🎉 Research intern in YouTu Lab, Tencent, mentored by Researcher <a href="https://scholar.google.com.hk/citations?hl=zh-CN&user=xiK4nFUAAAAJ">Yabiao Wang</a> and <a href="https://tyshiwo.github.io">Dr. Ying Tai</a>. </li>
    <li><i>2020.07.03</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>ECCV 2020 as spotlight presentation</strong>. </li>
    <li><i>2020.04.01</i>: &nbsp;🎉🎉🎉 Pursuing Ph.D. in Zhejiang University, under the supervision of <a href="https://april.zju.edu.cn/our-team">Prof. Yong Liu</a>. 🔭🔭🔭 </li>
    <li><i>2020.02.27</i>: &nbsp;🎉🎉🎉 Two papers are accepted by <strong>CVPR 2020</strong>. </li>
    <li><i>2020.01.25</i>: &nbsp;🎉🎉🎉 One paper is accepted by <strong>ICASSP 2020</strong>. </li>
  </ol>
</div>
<!-- </details> -->



# 📝 Publications 
<h2>
  <a href="#pub2025"><u>2025</u></a>&nbsp;
  <a href="#pub2024"><u>2024</u></a>&nbsp;
	<a href="#pub2023"><u>2023</u></a>&nbsp;
	<a href="#pub2022"><u>2022</u></a>&nbsp;
  <a href="#pub2021"><u>2021</u></a>&nbsp;
  <a href="#pub2020"><u>2020</u></a>&nbsp;
  <!-- <a href="#arxiv"><u>arXiv</u></a>&nbsp; -->
  <a href="#challenges"><u>Challenges</u></a>&nbsp;
</h2>
<span style="color:#b02418; font-weight:bold;">*</span> Joint first author | <span style="color:#b02418; font-weight:bold;">#</span> Corresponding author | <span style="color:#b02418; font-weight:bold;">†</span> Project lead <br> 
<strong>Summary:</strong> IJCV (2), TPAMI (2), TIP (1),  CVIU (1) <br>
<strong>Summary:</strong> CVPR (23), ICCV (5), ECCV (10), NeurIPS (4), ICLR (1) <br>

<h2 id="pub2025" style="color: #2c4a88; padding-top: 60px; margin-top: -60px;">2025</h2>
<ol reversed>
  <li id="2025-12"> 
    Improving Autoregressive Visual Generation with Cluster-Oriented Token Prediction <a href="https://arxiv.org/abs/2501.00880">[Paper]</a> <a href="https://github.com/sjtuplayer/IAR">[Code]</a> <br> 
    Teng Hu, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Ran Yi, Jieyu Weng, Yabiao Wang, Xianfang Zeng, Zhucun Xue, and Lizhuang Ma <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2025.
  </li>
  <li id="2025-11"> 
    MobileMamba: Lightweight Multi-Receptive Visual Mamba Network <a href="https://arxiv.org/abs/2411.15941">[Paper]</a> <a href="https://github.com/lewandofskee/MobileMamba">[Code]</a> <br> 
    Haoyang He, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Yuxuan Cai, Hongxu Chen, Xiaobin Hu, Zhenye Gan, Yabiao Wang, Chengjie Wang, Yunsheng Wu, and Lei Xie <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2025.
  </li>
  <li id="2025-10"> 
    GroundingFace: Fine-grained Face Understanding via Pixel Grounding Multimodal Large Language Model <br> 
    Yue Han, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang†</span>, Junwei Zhu, Runze Hou, Xiaozhong Ji, Chuming Lin, Xiaobin Hu, Zhucun Xue, and Yong Liu <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2025.
  </li>
  <li id="2025-9"> 
    TIMotion: Temporal and Interactive Framework for Efficient Human-Human Motion Generation <a href="https://arxiv.org/abs/2408.17135">[Paper]</a> <a href="https://aigc-explorer.github.io/TIMotion-page/">[Project]</a> <a href="https://github.com/AIGC-Explorer/TIMotion">[Code]</a> <br> 
    Yabiao Wang, Shuo Wang, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Ke Fan, Jiafu Wu, Zhucun Xue, and Yong Liu <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2025.
  </li>
  <li id="2025-8"> 
    AnyMaker: Zero-shot General Object Customization via Decoupled Dual-Level ID Injection <a href="https://arxiv.org/abs/2406.11643v3">[Paper]</a> <a href="https://lingjiekong-fdu.github.io/">[Project]</a> <a href="https://github.com/LingjieKong-fdu/CustAny">[Code]</a> <br> 
    Lingjie Kong, Kai Wu, Xiaobin Hu, Wenhui Han, Jinlong Peng, Chengming Xu, Donghao Luo, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Chengjie Wang, and Yanwei Fu <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2025.
  </li>
  <li id="2025-7"> 
    Real-IAD D³: A Real-World 2D/Pseudo-3D/3D Dataset for Industrial Anomaly Detection <br> 
    Wenbing Zhu, Lidong Wang, Ziqing Zhou, Chengjie Wang, Yurui Pan, Ruoyi Zhang, Zhuhao Chen, Linjie Cheng, Bin-Bin Gao, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Zhenye Gan, Yuxie Wang, Yulong Chen, Shuguang Qian, Mingmin Chi, Bo Peng, and Lizhuang Ma <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2025.
  </li>
  <li id="2025-6"> 
    SVFR: A Unified Framework for Generalized Video Face Restoration <a href="https://arxiv.org/abs/2501.01235">[Paper]</a> <a href="https://wangzhiyaoo.github.io/SVFR/">[Project]</a> <a href="https://github.com/wangzhiyaoo/SVFR">[Code]</a> <br> 
    Zhiyao Wang, Xu Chen, Chengming Xu, Junwei Zhu, Xiaobin Hu, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Chengjie Wang, Yuqi Liu, Yiyi Zhou, and Rongrong Ji <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2025.
  </li>
  <li id="2025-5"> 
    OSV: One Step is Enough for High-Quality Image to Video Generation <a href="https://arxiv.org/abs/2409.11367">[Paper]</a> <br> 
    Xiaofeng Mao, Zhengkai Jiang, Fu-Yun Wang, Wenbing Zhu, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Hao Chen, Mingmin Chi, and Yabiao Wang <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2025.
  </li>
  <li id="2025-4"> 
    Unveil Inversion and Invariance in Flow Transformer for Versatile Image Editing <a href="https://arxiv.org/abs/2411.15843">[Paper]</a> <a href="https://pengchengpcx.github.io/EditFT/">[Project]</a> <a href="https://github.com/Pengchengpcx/FTEdit">[Code]</a> <br> 
    Pengcheng Xu, Boyuan Jiang, Xiaobin Hu, Donghao Luo, Qingdong He, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Chengjie Wang, Yunsheng Wu, Charles Ling, and Boyu Wang <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2025.
  </li>
  <li id="2025-3"> 
    Sonic: Shifting Focus to Global Audio Perception in Portrait Animation <a href="https://arxiv.org/abs/2411.16331">[Paper]</a> <a href="https://jixiaozhong.github.io/Sonic/">[Project]</a> <a href="https://github.com/jixiaozhong/Sonic">[Code]</a> <br> 
    Xiaozhong Ji, Xiaobin Hu, Zhihong Xu, Junwei Zhu, Chuming Lin, Qingdong He, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Donghao Luo, Yi Chen, Qin Lin, Qinglin Lu, and Chengjie Wang <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2025.
  </li>
  <li id="2025-2"> 
    Exploring Plain ViT Reconstruction for Multi-class Unsupervised Anomaly Detection <a href="https://arxiv.org/abs/2312.07495">[Paper]</a> <a href="https://zhangzjn.github.io/projects/ViTAD/">[Project]</a> <a href="https://github.com/zhangzjn/ADer">[Code]</a> <br> 
    <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Xuhai Chen, Yabiao Wang, Chengjie Wang, Yong Liu, Xiangtai Li, Ming-Hsuan Yang, and Dacheng Tao <br>
    <i>Computer Vision and Image Understanding <strong>(CVIU).</strong></i> 2025.
  </li>
  <li id="2025-1"> 
    SaRA: High-Efficient Diffusion Model Fine-tuning with Progressive Sparse Low-Rank Adaptation <a href="https://arxiv.org/abs/2409.06633">[Paper]</a> <a href="https://sjtuplayer.github.io/projects/SaRA/">[Project]</a> <a href="https://github.com/sjtuplayer/SaRA">[Code]</a> <br> 
    Teng Hu, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Ran Yi, Hongrui Huang, Yabiao Wang, and Lizhuang Ma <br>
    <i>International Conference on Learning Representations <strong>(ICLR).</strong></i> 2025.
  </li>
</ol>

<h2 id="pub2024" style="color: #2c4a88; padding-top: 60px; margin-top: -60px;">2024</h2>
<ol reversed>
  <li id="2024-26"> 
    Explore In-Context Segmentation via Latent Diffusion Models <a href="https://arxiv.org/abs/2404.06564">[Paper]</a> <a href="https://wang-chaoyang.github.io/project/refldmseg/">[Project]</a> <a href="https://github.com/wang-chaoyang/RefLDMSeg">[Code]</a> <br> 
    Chaoyang Wang, Xiangtai Li, Henghui Ding, Lu Qi, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Yunhai Tong, Chen Change Loy, and Shuicheng Yan <br>
    <i>Association for the Advancement of Artificial Intelligence <strong>(AAAI).</strong></i> 2025.
  </li>
  <li id="2024-25"> 
    Point Cloud Mamba: Point Cloud Learning via State Space Model <a href="https://arxiv.org/abs/2403.00762">[Paper]</a> <a href="https://github.com/zhang-tao-whu/PCM">[Code]</a> <br> 
    Tao Zhang, Haobo Yuan, Lu Qi, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Qianyu Zhou, Shunping Ji, Shuicheng Yan, and Xiangtai Li <br>
    <i>Association for the Advancement of Artificial Intelligence <strong>(AAAI).</strong></i> 2025.
  </li>
  <li id="2024-24"> 
    ID-Sculpt: ID-aware 3D Head Generation from Single In-the-wild Portrait Image <a href="https://arxiv.org/abs/2406.16710">[Paper]</a> <a href="https://jinkun-hao.github.io/Portrait3D">[Project]</a> <a href="https://github.com/jinkun-hao/Portrait3D">[Code]</a> <br> 
    Jinkun Hao, Junshu Tang, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Ran Yi, Yijia Hong, Moran Li, Weijian Cao, Yating Wang, Chengjie Wang, and Lizhuang Ma <br>
    <i>Association for the Advancement of Artificial Intelligence <strong>(AAAI).</strong></i> 2025.
  </li>
  <li id="2024-23"> 
    PointRWKV: Efficient RWKV-Like Model for Hierarchical Point Cloud Learning <a href="https://arxiv.org/abs/2405.15214">[Paper]</a> <a href="https://github.com/hithqd/PointRWKV">[Code]</a> <br> 
    Qingdong He, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Jinlong Peng, Haoyang He, Xiangtai Li, Yabiao Wang, and Chengjie Wang <br>
    <i>Association for the Advancement of Artificial Intelligence <strong>(AAAI).</strong></i> 2025.
  </li>
  <li id="2024-22"> 
    MambaAD: Exploring State Space Models for Multi-class Unsupervised Anomaly Detection <a href="https://arxiv.org/abs/2404.06564">[Paper]</a> <a href="https://lewandofskee.github.io/projects/MambaAD/">[Project]</a> <a href="https://github.com/lewandofskee/MambaAD">[Code]</a> <br> 
    Haoyang He, Yuhu Bai, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang#</span>, Qingdong He, Hongxu Chen, Zhenye Gan, Chengjie Wang, Xiangtai Li, Guanzhong Tian, and Lei Xie <br>
    <i>Conference on Neural Information Processing Systems <strong>(NeurIPS).</strong></i> 2024.
  </li>
  <li id="2024-21"> 
    MotionBooth: Motion-Aware Customized Text-to-Video Generation <a href="https://arxiv.org/abs/2406.17758">[Paper]</a> <a href="https://jianzongwu.github.io/projects/motionbooth/">[Project]</a> <a href="https://github.com/jianzongwu/MotionBooth">[Code]</a> <br> 
    Jianzong Wu, Xiangtai Li, Yanhong Zeng, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen <br>
    <i>Conference on Neural Information Processing Systems <strong>(NeurIPS).</strong></i> 2024.
  </li>
  <li id="2024-20"> 
    Dual-path Frequency Discriminators for Few-shot Anomaly Detection <a href="https://arxiv.org/abs/2403.04151">[Paper]</a> <a href="https://github.com/yuhbai/DFD">[Code]</a> <br> 
    Yuhu Bai*, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Zhaofeng Chen, Yuhang Dong, Yunkang Cao, and Guanzhong Tian <br>
    <i>Knowledge-Based Systems <strong>(KBS).</strong></i> 2024.
  </li>
  <li id="2024-19"> 
    MDT-A2G: Exploring Masked Diffusion Transformers for Co-Speech Gesture Generation <a href="https://arxiv.org/abs/2408.03312">[Paper]</a> <a href="https://xiaofenmao.github.io/web-project/MDT-A2G/">[Project]</a> <br> 
    Xiaofeng Mao, Zhengkai Jiang, Qilin Wang, Chencan Fu, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Jiafu Wu, Yabiao Wang, Chengjie Wang, Wei Li, and Mingmin Chi <br>
    <i>ACM Multimedia <strong>(ACM MM).</strong></i> 2024.
  </li>
  <li id="2024-18"> 
    MambaGesture: Enhancing Co-Speech Gesture Generation with Mamba and Disentangled Multi-Modality Fusion <a href="https://arxiv.org/abs/2407.19976">[Paper]</a> <a href="https://fcchit.github.io/mambagesture/">[Project]</a> <br> 
    Chencan Fu, Yabiao Wang, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Zhengkai Jiang, Xiaofeng Mao, Jiafu Wu, Weijian Cao, Chengjie Wang, Yanhao Ge, and Yong Liu <br>
    <i>ACM Multimedia <strong>(ACM MM).</strong></i> 2024.
  </li>
  <li id="2024-17"> 
    MotionMaster: Training-free Camera Motion Transfer For Video Generation <a href="https://arxiv.org/abs/2404.15789">[Paper]</a> <a href="https://github.com/sjtuplayer/MotionMaster">[Code]</a> <a href="https://sjtuplayer.github.io/projects/MotionMaster">[Project]</a> <br> 
    Teng Hu*, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Ran Yi, Yating Wang, Hongrui Huang, Jieyu Weng, Yabiao Wang, and Lizhuang Ma <br>
    <i>ACM Multimedia <strong>(ACM MM).</strong></i> 2024.
  </li>
  <li id="2024-16"> 
    DiffuMatting: Synthesizing Arbitrary Objects with Matting-level Annotation <a href="https://arxiv.org/abs/2403.06168">[Paper]</a> <br> 
    Xiaobin Hu, Xu Peng, Donghao Luo, Xiaozhong Ji, Jinlong Peng, Zhengkai Jiang, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Taisong Jin, Chengjie Wang, and Rongrong Ji <br>
    <i>European Conference on Computer Vision <strong>(ECCV).</strong></i> 2024.
  </li>
  <li id="2024-15"> 
    Face Adapter for Pre-Trained Diffusion Models with Fine-Grained ID and Attribute Control <a href="https://arxiv.org/abs/2405.12970">[Paper]</a> <a href="https://github.com/FaceAdapter/Face-Adapter">[Code]</a> <a href="https://faceadapter.github.io/face-adapter.github.io">[Project]</a> <br> 
    Yue Han, Junwei Zhu, Keke He, Xu Chen, Yanhao Ge, Wei Li, Xiangtai Li, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang#</span>, Chengjie Wang, and Yong Liu <br>
    <i>European Conference on Computer Vision <strong>(ECCV).</strong></i> 2024.
  </li>
  <li id="2024-14"> 
    TexDreamer: Towards Zero-Shot High-Fidelity 3D Human Texture Generation <a href="https://arxiv.org/abs/2403.12906">[Paper]</a> <a href="https://github.com/ggxxii/texdreamer">[Code]</a> <a href="https://ggxxii.github.io/texdreamer">[Project]</a> <br> 
    Yufei Liu, Junwei Zhu, Junshu Tang, Shijie Zhang, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Weijian Cao, Chengjie Wang, Yunsheng Wu, and Dongjin Huang <br>
    <i>European Conference on Computer Vision <strong>(ECCV).</strong></i> 2024.
  </li>
  <li id="2024-13"> 
    AdaCLIP: Adapting CLIP with Hybrid Learnable Prompts for Zero-Shot Anomaly Detection <a href="">[Paper]</a> <br> 
    Yunkang Cao, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Luca Frittoli, Yuqi Cheng, Weiming Shen, and Giacomo Boracchi <br>
    <i>European Conference on Computer Vision <strong>(ECCV).</strong></i> 2024.
  </li>
  <li id="2024-12"> 
    FreeMotion: A Unified Framework for Number-free Text-to-Motion Synthesis <a href="https://arxiv.org/abs/2405.15763">[Paper]</a> <br> 
    Ke Fan, Junshu Tang, Weijian Cao, Ran Yi, Moran Li, Jingyu Gong, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Yabiao Wang, Chengjie Wang, and Lizhuang Ma <br>
    <i>European Conference on Computer Vision <strong>(ECCV).</strong></i> 2024.
  </li>
  <li id="2024-11"> 
    Self-supervised Feature Adaptation for 3D Industrial Anomaly Detection <a href="https://arxiv.org/abs/2401.03145">[Paper]</a> <br> 
    Yuanpeng Tu, Boshen Zhang, Liang Liu, Yuxi Li, Xuhai Chen, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Yabiao Wang, Chengjie Wang, and Cai Rong Zhao <br>
    <i>European Conference on Computer Vision <strong>(ECCV).</strong></i> 2024.
  </li>
  <li id="2024-10"> 
    Reference Twice: A Simple and Unified Baseline for Few-Shot Instance Segmentation <a href="https://arxiv.org/abs/2301.01156">[Paper]</a> <a href="https://github.com/hanyue1648/RefT">[Code]</a> <br> 
    Yue Han*, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Zhucun Xue, Chao Xu, Xintian Shen, Yabiao Wang, Chengjie Wang, Yong Liu, and Xiangtai Li <br>
    <i>Transactions on Pattern Analysis and Machine Intelligence <strong>(T-PAMI).</strong></i> 2024.
  </li>
  <li id="2024-9"> 
    GPT-4V-AD: Exploring Grounding Potential of VQA-oriented GPT-4V for Zero-shot Anomaly Detection <a href="https://arxiv.org/abs/2311.02612">[Paper]</a> <a href="https://github.com/zhangzjn/GPT-4V-AD">[Code]</a> <br> 
   <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Haoyang He, Xuhai Chen, Zhucun Xue, Yabiao Wang, Chengjie Wang, Lei Xie, and Yong Liu <br>
    <i>International Joint Conference on Artificial Intelligence <strong>(IJCAI).</strong></i> 2024.
  </li>
  <li id="2024-8"> 
    CLIP-AD: A Language-Guided Staged Dual-Path Model for Zero-shot Anomaly Detection <a href="https://arxiv.org/abs/2311.00453">[Paper]</a> <br> 
   Xuhai Chen, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Guanzhong Tian, Haoyang He, Wuhao Zhang, Yabiao Wang, Chengjie Wang, and Yong Liu <br>
    <i>International Joint Conference on Artificial Intelligence <strong>(IJCAI).</strong></i> 2024.
  </li>
  <li id="2024-7"> 
    UniM-OV3D: Uni-Modality Open-Vocabulary 3D Scene Understanding with Fine-Grained Feature Representation <a href="https://arxiv.org/abs/2401.11395">[Paper]</a> <a href="https://github.com/hithqd/UniM-OV3D">[Code]</a> <br> 
   Qingdong He, Jinlong Peng, Zhengkai Jiang, Kai Wu, Xiaozhong Ji, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang#</span>, Yabiao Wang, Chengjie Wang, Mingang Chen, and Yunsheng Wu <br>
    <i>International Joint Conference on Artificial Intelligence <strong>(IJCAI).</strong></i> 2024.
  </li>
  <li id="2024-6"> 
    Real-IAD: A Real-World Multi-View Dataset for Benchmarking Versatile Industrial Anomaly Detection <a href="https://arxiv.org/abs/2403.12580">[Paper]</a> <a href="https://github.com/TencentYoutuResearch/AnomalyDetection_Real-IAD">[Code]</a> <a href="https://realiad4ad.github.io/Real-IAD">[Project]</a> <br> 
    Chengjie Wang, Wenbing Zhu, Bin-Bin Gao, Zhenye Gan, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Zhihao Gu, Shuguang Qian, Mingang Chen, and Lizhuang Ma <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2024.
  </li>
  <li id="2024-5"> 
    Towards Language-Driven Video Inpainting Via Multimodal Large Language Models <a href="https://arxiv.org/abs/2401.10226">[Paper]</a> <a href="https://github.com/jianzongwu/Language-Driven-Video-Inpainting">[Code]</a> <a href="https://jianzongwu.github.io/projects/rovi">[Project]</a> <br> 
    Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, and Chen Change Loy <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2024.
  </li>
  <li id="2024-4"> 
    SuperSVG: Superpixel-Based Scalable Vector Graphics Synthesis <a href="https://arxiv.org/abs/2406.09794">[Paper]</a> <a href="https://github.com/sjtuplayer/SuperSVG">[Code]</a> <br> 
    Teng Hu, Ran Yi, Baihong Qian, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Paul Rosin, and Yukun Lai <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2024.
  </li>
  <li id="2024-3"> 
    PortraitBooth: A Versatile Portrait Model for Fast Identity-preserved Personalization <a href="https://arxiv.org/abs/2312.06354">[Paper]</a> <a href="https://portraitbooth.github.io">[Project]</a> <br> 
    Xu Peng, Junwei Zhu, Boyuan Jiang, Ying Tai, Donghao Luo, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Wei Lin, Taisong Jin, Chengjie Wang, and Rongrong Ji <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2024.
  </li>
  <li id="2024-2"> 
    EATFormer: Improving Vision Transformer Inspired by Evolutionary Algorithm <a href="https://arxiv.org/abs/2206.09325">[Paper]</a> <a href="https://github.com/zhangzjn/EATFormer">[Code]</a> <br> 
    <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Xiangtai Li, Yabiao Wang, Chengjie Wang, Yibo Yang, Yong Liu, and Dacheng Tao <br>
    <i>International Journal of Computer Vision <strong>(IJCV).</strong></i> 2024.
  </li>
  <li id="2024-1"> 
    Towards open vocabulary learning: A survey <a href="https://arxiv.org/abs/2306.15880">[Paper]</a> <a href="https://github.com/jianzongwu/Awesome-Open-Vocabulary">[Code]</a> <br> 
    Jianzong Wu, Xiangtai Li, Shilin Xu, Haobo Yuan, Henghui Ding, Yibo Yang, Xia Li, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Yunhai Tong, Xudong Jiang, Bernard Ghanem, and Dacheng Tao <br>
    <i>Transactions on Pattern Analysis and Machine Intelligence <strong>(T-PAMI).</strong></i> 2024.
  </li>
</ol>

<h2 id="pub2023" style="color: #2c4a88; padding-top: 60px; margin-top: -60px;">2023</h2>
<ol reversed>
  <li id="2023-19"> 
    Hear to Segment: Unmixing the Audio to Guide the Semantic Segmentation <a href="https://ieeexplore.ieee.org/abstract/document/10446516">[Paper]</a> <br> 
    Yuhang Ling, Yuxi Li, Zhenye Gan, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Mingmin Chi, and Yabiao Wang <br>
    <i>International Conference on Acoustics, Speech, and Signal Processing <strong>(ICASSP).</strong></i> 2024.
  </li>
  <li id="2023-18"> 
    DiAD: A Diffusion-based Framework for Multi-class Anomaly Detection <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28690">[Paper]</a> <a href="https://github.com/lewandofskee/DiAD">[Code]</a> <a href="https://lewandofskee.github.io/projects/diad">[Project]</a> <br> 
    Haoyang He*, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Hongxu Chen, Xuhai Chen, Zhishan Li, Xu Chen, Yabiao Wang, Chengjie Wang, and Lei Xie <br>
    <i>Association for the Advancement of Artificial Intelligence <strong>(AAAI).</strong></i> 2024.
  </li>
  <li id="2023-17"> 
    Rethinking Reverse Distillation for Multi-Modal Anomaly Detection <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28687">[Paper]</a> <br> 
    Zhihao Gu, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Liang Liu, Xu Chen, Jinlong Peng, Zhenye Gan, Guannan Jiang, Annan Shu, Yabiao Wang, and Lizhuang Ma <br>
    <i>Association for the Advancement of Artificial Intelligence <strong>(AAAI).</strong></i> 2024.
  </li>
  <li id="2023-16"> 
    Self-supervised Likelihood Estimation with Energy Guidance for Anomaly Segmentation in Urban Scenes <a href="https://ojs.aaai.org/index.php/AAAI/article/view/30162">[Paper]</a> <br> 
    Yuanpeng Tu, Yuxi Li, Boshen Zhang, Liang Liu, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Yabiao Wang, and Cairong Zhao <br>
    <i>Association for the Advancement of Artificial Intelligence <strong>(AAAI).</strong></i> 2024.
  </li>
  <li id="2023-15"> 
    AnomalyDiffusion: Few-Shot Anomaly Image Generation with Diffusion Model <a href="https://ojs.aaai.org/index.php/AAAI/article/view/28696">[Paper]</a> <a href="https://github.com/sjtuplayer/anomalydiffusion">[Code]</a> <a href="https://sjtuplayer.github.io/anomalydiffusion-page">[Project]</a> <br> 
    Teng Hu*, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Ran Yi, Yuzhen Du, Xu Chen, Liang Liu, Yabiao Wang, and Chengjie Wang <br>
    <i>Association for the Advancement of Artificial Intelligence <strong>(AAAI).</strong></i> 2024.
  </li>
  <li id="2023-14"> 
    PVG: Progressive Vision Graph for Vision Recognition <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3612122">[Paper]</a> <br> 
    Jiafu Wu, Jian Li, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Boshen Zhang, Mingmin Chi, Yabiao Wang, and Chengjie Wang <br>
    <i>ACM Multimedia <strong>(ACM MM).</strong></i> 2023.
  </li>
  <li id="2023-13"> 
    Toward High Quality Facial Representation Learning <a href="https://dl.acm.org/doi/abs/10.1145/3581783.3611999">[Paper]</a> <a href="https://github.com/nomewang/MCF">[Code]</a> <br> 
    Yue Wang, Jinlong Peng, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Ran Yi, Liang Liu, Yabiao Wang, and Chengjie Wang <br>
    <i>ACM Multimedia <strong>(ACM MM).</strong></i> 2023.
  </li>
  <li id="2023-12"> 
    Rethinking Mobile Block for Efficient Attention-based Models <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_Rethinking_Mobile_Block_for_Efficient_Attention-based_Models_ICCV_2023_paper.html">[Paper]</a> <a href="https://github.com/zhangzjn/EMO">[Code]</a> <br> 
    <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Xiangtai Li, Jian Li, Liang Liu, Zhucun Xue, Boshen Zhang, Zhengkai Jiang, Tianxin Huang, Yabiao Wang, and Chengjie Wang <br>
    <i>International Conference on Computer Vision <strong>(ICCV).</strong></i> 2023.
  </li>
  <li id="2023-11"> 
    Remembering Normality: Memory-guided Knowledge Distillation for Unsupervised Anomaly Detection <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Gu_Remembering_Normality_Memory-guided_Knowledge_Distillation_for_Unsupervised_Anomaly_Detection_ICCV_2023_paper.html">[Paper]</a> <br> 
    Zhihao Gu, Liang Liu, Xu Chen, Ran Yi, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Yabiao Wang, Chengjie Wang, Annan Shu, Guannan Jiang, and Lizhuang Ma <br>
    <i>International Conference on Computer Vision <strong>(ICCV).</strong></i> 2023.
  </li>
  <li id="2023-10"> 
    Learning Global-aware Kernel for Image Harmonization <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Shen_Learning_Global-aware_Kernel_for_Image_Harmonization_ICCV_2023_paper.html">[Paper]</a> <br> 
    Xintian Shen*, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Jun Chen, Shipeng Bai, Yue Han, Yabiao Wang, Chengjie Wang, and Yong Liu <br>
    <i>International Conference on Computer Vision <strong>(ICCV).</strong></i> 2023.
  </li>
  <li id="2023-9"> 
    Phasic Content Fusing Diffusion Model with Directional Distribution Consistency for Few-Shot Model Adaption <a href="https://openaccess.thecvf.com/content/ICCV2023/html/Hu_Phasic_Content_Fusing_Diffusion_Model_with_Directional_Distribution_Consistency_for_ICCV_2023_paper.html">[Paper]</a> <br> 
    Teng Hu*, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Liang Liu, Ran Yi, Siqi Kou, Haokun Zhu, Xu Chen, Yabiao Wang, Chengjie Wang, and Lizhuang Ma <br>
    <i>International Conference on Computer Vision <strong>(ICCV).</strong></i> 2023.
  </li>
  <li id="2023-8"> 
    SFNet: Faster and Accurate Semantic Segmentation Via Semantic Flow <a href="https://arxiv.org/abs/2207.04415">[Paper]</a> <a href="https://github.com/lxtGH/SFSegNets">[Code]</a> <br> 
    Xiangtai Li, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Yibo Yang, Guangliang Cheng, Kuiyuan Yang, Yunhai Tong, and Dacheng Tao <br>
    <i>International Journal of Computer Vision <strong>(IJCV).</strong></i> 2023.
  </li>
  <li id="2023-7"> 
    Omni-frequency Channel-selection Representations for Unsupervised Anomaly Detection <a href="https://ieeexplore.ieee.org/abstract/document/10192551/">[Paper]</a> <a href="https://github.com/zhangzjn/OCR-GAN">[Code]</a> <br> 
    Yufei Liang*, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Shiwei Zhao, Runze Wu, Yong Liu, and Shuwen Pan <br>
    <i>Transactions on Image Processing <strong>(TIP).</strong></i> 2023.
  </li>
  <li id="2023-6"> 
    Multimodal Industrial Anomaly Detection via Hybrid Fusion <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Multimodal_Industrial_Anomaly_Detection_via_Hybrid_Fusion_CVPR_2023_paper.html">[Paper]</a> <a href="https://github.com/nomewang/M3DM">[Code]</a> <br> 
    Yue Wang, Jinlong Peng, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Ran Yi, Yabiao Wang, and Chengjie Wang <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2023.
  </li>
  <li id="2023-5"> 
    Learning To Measure the Point Cloud Reconstruction Loss in a Representation Space <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Learning_To_Measure_the_Point_Cloud_Reconstruction_Loss_in_a_CVPR_2023_paper.html">[Paper]</a> <br> 
    Tianxin Huang, Zhonggan Ding, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Ying Tai, Zhenyu Zhang, Mingang Chen, Chengjie Wang, and Yong Liu <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2023.
  </li>
  <li id="2023-4"> 
    MixTeacher: Mining Promising Labels With Mixed Scale Teacher for Semi-Supervised Object Detection <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Liu_MixTeacher_Mining_Promising_Labels_With_Mixed_Scale_Teacher_for_Semi-Supervised_CVPR_2023_paper.html">[Paper]</a> <a href="https://github.com/lliuz/MixTeacher">[Code]</a> <br> 
    Liang Liu, Boshen Zhang, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Wuhao Zhang, Zhenye Gan, Guanzhong Tian, Wenbing Zhu, Yabiao Wang, and Chengjie Wang <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2023.
  </li>
  <li id="2023-3"> 
    Better "CMOS" Produces Clearer Images: Learning Space-Variant Blur Estimation for Blind Image Super-Resolution <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Better_CMOS_Produces_Clearer_Images_Learning_Space-Variant_Blur_Estimation_for_CVPR_2023_paper.html">[Paper]</a> <br> 
    Xuhai Chen*, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Chao Xu, Yabiao Wang, Chengjie Wang, and Yong Liu <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2023.
  </li>
  <li id="2023-2"> 
    High-Fidelity Generalized Emotional Talking Face Generation With Multi-Modal Emotion Space Learning <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Xu_High-Fidelity_Generalized_Emotional_Talking_Face_Generation_With_Multi-Modal_Emotion_Space_CVPR_2023_paper.html">[Paper]</a> <br> 
    Chao Xu, Junwei Zhu, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Yue Han, Wenqing Chu, Ying Tai, Chengjie Wang, Zhifeng Xie, and Yong Liu <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2023.
  </li>
  <li id="2023-1"> 
    Learning With Noisy Labels via Self-Supervised Adversarial Noisy Masking <a href="https://openaccess.thecvf.com/content/CVPR2023/html/Tu_Learning_With_Noisy_Labels_via_Self-Supervised_Adversarial_Noisy_Masking_CVPR_2023_paper.html">[Paper]</a> <a href="https://github.com/yuanpengtu/SANM">[Code]</a> <br> 
    Yuanpeng Tu, Boshen Zhang, Yuxi Li, Liang Liu, Jian Li, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Yabiao Wang, Chengjie Wang, and Cairong Zhao <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2023.
  </li>
</ol>

<h2 id="pub2022" style="color: #2c4a88; padding-top: 60px; margin-top: -60px;">2022</h2>
<ol reversed>
  <li id="2022-11"> 
    Calibrated Teacher for Sparsely Annotated Object Detection <a href="https://link.springer.com/chapter/10.1007/978-3-031-19784-0_4">[Paper]</a> <a href="https://github.com/whileherham/calibratedteacher">[Code]</a> <br> 
    Haohan Wang, Liang Liu, Boshen Zhang, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Wuhao Zhang, Zhenye Gan, Yabiao Wang, Chengjie Wang, and Haoqian Wang <br>
    <i>Association for the Advancement of Artificial Intelligence <strong>(AAAI).</strong></i> 2023.
  </li>
    <li id="2022-10"> 
    Fast Point Cloud Sampling Network <a href="https://www.sciencedirect.com/science/article/abs/pii/S016786552200335X">[Paper]</a> <br> 
    Tianxin Huang, Jun Chen, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Yong Liu, and Jie Liang <br>
    <i>Pattern Recognition Letters <strong>(PRL).</strong></i> 2023.
  </li>
  <li id="2022-9"> 
    3QNet: 3D Point Cloud Geometry Quantization Compression Network <a href="https://dl.acm.org/doi/abs/10.1145/3550454.3555481">[Paper]</a> <br> 
    Tianxin Huang, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Jun Chen, Zhonggan Ding, Ying Tai, Zhenyu Zhang, Chengjie Wang, and Yong Liu <br>
    <i>ACM Transactions on Graphics <strong>(ACM TOG).</strong></i> 2022.
  </li>
  <li id="2022-8"> 
    Adaptive Recurrent Forward Network for Dense Point Cloud Completion <a href="https://ieeexplore.ieee.org/abstract/document/9864273">[Paper]</a> <br> 
    Tianxin Huang, Hao Zou, Jinhao Cui, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Xuemeng Yang, Lin Li, and Yong Liu <br>
    <i>Transactions on Multimedia <strong>(TMM).</strong></i> 2022.
  </li>
  <li id="2022-7"> 
    Designing One Unified Framework for High-Fidelity Face Reenactment and Swapping <a href="https://link.springer.com/chapter/10.1007/978-3-031-19784-0_4">[Paper]</a> <a href="https://github.com/xc-csc101/UniFace">[Code]</a> <br> 
    Chao Xu*, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Yue Han, Guanzhong Tian, Xianfang Zeng, Ying Tai, Yabiao Wang, Chengjie Wang, and Yong Liu <br>
    <i>European Conference on Computer Vision <strong>(ECCV).</strong></i> 2022.
  </li>
  <li id="2022-6"> 
    Resolution-free Point Cloud Sampling Network with Data Distillation <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/4326_ECCV_2022_paper.php">[Paper]</a> <a href="https://github.com/Tianxinhuang/PCDNet">[Code]</a> <br> 
    Tianxin Huang*, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Jun Chen, Yuang Liu, and Yong Liu <br>
    <i>European Conference on Computer Vision <strong>(ECCV).</strong></i> 2022.
  </li>
  <li id="2022-5"> 
    Learning to Train a Point Cloud Reconstruction Network without Matching <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/html/1235_ECCV_2022_paper.php">[Paper]</a> <a href="https://github.com/Tianxinhuang/PCLossNet">[Code]</a> <br> 
    Tianxin Huang, Xuemeng Yang, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Jinhao Cui, Hao Zou, Jun Chen, Xiangrui Zhao, and Yong Liu <br>
    <i>European Conference on Computer Vision <strong>(ECCV).</strong></i> 2022.
  </li>
  <li id="2022-4"> 
    Multilevel Spatial-Temporal Feature Aggregation for Video Object Detection <a href="https://ieeexplore.ieee.org/abstract/document/9797768">[Paper]</a> <br> 
    Chao Xu, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Mengmeng Wang, Guanzhong Tian, and Yong Liu <br>
    <i>Transactions on Circuits and Systems for Video Technology <strong>(TCSVT).</strong></i> 2022.
  </li>
  <li id="2022-3"> 
    Learning Hierarchical and Efficient Person Re-Identification for Robotic Navigation <a href="https://link.springer.com/article/10.1007/s41315-021-00167-2">[Paper]</a> <br> 
    <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Chao Xu, Xiangrui Zhao, Liang Liu, Yong Liu, Jinqiang Yao, and Zaisheng Pan <br>
    <i>International Journal of Intelligent Robotics and Applications <strong>(IJIRA, Best Paper Reward Nomination).</strong></i> 2022.
  </li>
  <li id="2022-2"> 
    Iterative Few-shot Semantic Segmentation from Image Label Text <a href="https://arxiv.org/abs/2303.05646">[Paper]</a> <a href="https://github.com/Whileherham/IMR-HSNet">[Code]</a> <br> 
    Haohan Wang, Liang Liu, Wuhao Zhang, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Zhenye Gan, Yabiao Wang, Chengjie Wang, and Haoqian Wang <br>
    <i>International Joint Conference on Artificial Intelligence <strong>(IJCAI).</strong></i> 2022.
  </li>
  <li id="2022-1"> 
    Region-Aware Face Swapping <a href="https://ieeexplore.ieee.org/abstract/document/9552566">[Paper]</a> <a href="https://github.com/zhangzjn/APB2Face">[Code]</a> <br> 
    Chao Xu*, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Miao Hua, Qian He, Zili Yi, and Yong Liu <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2022.
  </li>
</ol>

<h2 id="pub2021" style="color: #2c4a88; padding-top: 60px; margin-top: -60px;">2021</h2>
<ol reversed>
  <li id="2021-5"> 
    SCSNet: An Efficient Paradigm for Learning Simultaneously Image Colorization and Super-Resolution <a href="https://ojs.aaai.org/index.php/AAAI/article/view/20236">[Paper]</a> <br> 
    <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Chao Xu, Jian Li, Yue Han, Yabiao Wang, Ying Tai, and Yong Liu <br>
    <i>Association for the Advancement of Artificial Intelligence <strong>(AAAI).</strong></i> 2022.
  </li>
  <li id="2021-4"> 
    Analogous to Evolutionary Algorithm: Designing a Unified Sequence Model <a href="https://proceedings.neurips.cc/paper/2021/hash/e02e27e04fdff967ba7d76fb24b8069d-Abstract.html">[Paper]</a> <a href="https://github.com/TencentYoutuResearch/BaseArchitecture-EAT">[Code]</a> <br> 
    <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Chao Xu, Jian Li, Wenzhou Chen, Yabiao Wang, Ying Tai, Shuo Chen, Chengjie Wang, Feiyue Huang, and Yong Liu <br>
    <i>Conference on Neural Information Processing Systems <strong>(NeurIPS).</strong></i> 2021.
  </li>
  <li id="2021-3"> 
    Real-Time Audio-Guided Multi-Face Reenactment <a href="https://ieeexplore.ieee.org/abstract/document/9552566">[Paper]</a> <a href="https://github.com/zhangzjn/APB2FaceV2">[Code]</a> <br> 
    <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Xianfang Zeng, Chao Xu, and Yong Liu <br>
    <i>Signal Processing Letters <strong>(SPL).</strong></i> 2021.
  </li>
  <li id="2021-2"> 
    Adding Before Pruning: Sparse Filter Fusion for Deep Convolutional Neural Networks via Auxiliary Attention <a href="https://ieeexplore.ieee.org/abstract/document/9552566">[Paper]</a> <a href="https://github.com/zhangzjn/APB2FaceV2">[Code]</a> <br> 
    Guanzhong Tian, Yiran Sun, Yuang Liu, Xianfang Zeng, Mengmeng Wang, Yong Liu, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, and Jun Chen <br>
    <i>Transactions on Neural Networks and Learning Systems <strong>(TNNLS).</strong></i> 2021.
  </li>
  <li id="2021-1"> 
    RFNet: Recurrent Forward Network for Dense Point Cloud Completion <a href="https://openaccess.thecvf.com/content/ICCV2021/html/Huang_RFNet_Recurrent_Forward_Network_for_Dense_Point_Cloud_Completion_ICCV_2021_paper.html">[Paper]</a> <a href="https://github.com/Tianxinhuang/RFNet">[Code]</a> <br> 
    Tianxin Huang, Hao Zou, Jinhao Cui, Xuemeng Yang, Mengmeng Wang, Xiangrui Zhao, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Yi Yuan, Yifan Xu, and Yong Liu <br>
    <i>International Conference on Computer Vision <strong>(ICCV).</strong></i> 2021.
  </li>
</ol>

<h2 id="pub2020" style="color: #2c4a88; padding-top: 60px; margin-top: -60px;">2020</h2>
<ol reversed>
  <li id="2020-4"> 
    DTVNet: Dynamic Time-Lapse Video Generation via Single Still Image <a href="https://link.springer.com/chapter/10.1007/978-3-030-58558-7_18">[Paper]</a> <a href="https://github.com/zhangzjn/DTVNet">[Code]</a> <br> 
    <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span>, Chao Xu*, Liang Liu, Mengmeng Wang, Xia Wu, Yong Liu, and Yunliang Jiang <br>
    <i>European Conference on Computer Vision <strong>(ECCV).</strong></i> 2020.
  </li>
  <li id="2020-3"> 
    FReeNet: Multi-Identity Face Reenactment <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_FReeNet_Multi-Identity_Face_Reenactment_CVPR_2020_paper.html">[Paper]</a> <a href="https://github.com/zhangzjn/FReeNet">[Code]</a> <br> 
    <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Xianfang Zeng, Mengmeng Wang, Yusu Pan, Liang Liu, Yong Liu, Yu Ding, and Changjie Fan <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2020.
  </li>
  <li id="2020-2"> 
    Learning by Analogy: Reliable Supervision From Transformations for Unsupervised Optical Flow Estimation <a href="https://openaccess.thecvf.com/content_CVPR_2020/html/Liu_Learning_by_Analogy_Reliable_Supervision_From_Transformations_for_Unsupervised_Optical_CVPR_2020_paper.html">[Paper]</a> <a href="https://github.com/lliuz/ARFlow">[Code]</a> <br> 
    Liang Liu, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Ruifei He, Yong Liu, Yabiao Wang, Ying Tai, Donghao Luo, Chengjie Wang, Jilin Li, and Feiyue Huang <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR).</strong></i> 2020.
  </li>
  <li id="2020-1"> 
    APB2FACE: Audio-Guided Face Reenactment with Auxiliary Pose and Blink Signals <a href="https://ieeexplore.ieee.org/abstract/document/9052977">[Paper]</a> <a href="https://github.com/zhangzjn/APB2Face">[Code]</a> <br> 
    <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, Liang Liu, Zhucun Xue, and Yong Liu <br>
    <i>International Conference on Acoustics, Speech, and Signal Processing <strong>(ICASSP).</strong></i> 2020.
  </li>
  <li id="2019-1"> 
    Realistic Face Reenactment via Self-Supervised Disentangling of Identity and Pose <a href="https://ojs.aaai.org/index.php/AAAI/article/view/6970">[Paper]</a> <br> 
    Xianfang Zeng, Yusu Pan, Mengmeng Wang, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span>, and Yong Liu <br>
    <i>Association for the Advancement of Artificial Intelligence <strong>(AAAI).</strong></i> 2020.
  </li>
  <li id="2018-1"> 
    Learning-Based Hand Motion Capture and Understanding in Assembly Process <a href="https://ieeexplore.ieee.org/abstract/document/8566182">[Paper]</a> <br> 
    Liang Liu, Yong Liu, and <span style="color:#b02418; font-weight:bold;">Jiangning Zhang</span> <br>
    <i>Transactions on Industrial Electronics <strong>(TIE).</strong></i> 2018.
  </li>
</ol>


<h2 id="challenges" style="color: #2c4a88; padding-top: 60px; margin-top: -60px;">Challenges</h2>
<ol reversed>
  <li id="2018-1"> 
    1st place for <a href="(https://codalab.lisn.upsaclay.fr/competitions/12499#results)">[Zero-shot Track]</a> and 4th place for <a href="(https://codalab.lisn.upsaclay.fr/competitions/12500#results)">[Few-shot Track]</a> in <a href="(https://sites.google.com/view/vand-cvpr23/home)">[Visual Anomaly and Novelty Detection (VAND) 2023 Challenge]</a> by <strong>CVPR 2023</strong> <br>
    A Zero-/Few-Shot Anomaly Classification and Segmentation Method for CVPR 2023 VAND Workshop Challenge Tracks 1&2: 1st Place on Zero-shot AD and 4th Place on Few-shot AD <a href="https://arxiv.org/abs/2305.17382">[Paper]</a> <a href="https://github.com/ByChelsea/VAND-APRIL-GAN">[Code]</a> <br> 
    Xuhai Chen*, Yue Han*, <span style="color:#b02418; font-weight:bold;">Jiangning Zhang*</span> <br>
    <i>Computer Vision and Pattern Recognition <strong>(CVPR)</strong> VAND.</i> 2023.
  </li>
</ol>

<!-- # 🎖 Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  -->

# 📖 Educations
- *2020.04 - 2022.10*, M.D. 👉️ Ph.D. in College of Control Science and Engineering, Zhejiang University, Hangzhou, China. 
- *2017.09 - 2020.04*, Pursuing M.D. in College of Control Science and Engineering, Zhejiang University, Hangzhou, China.
- *2013.09 - 2017.06*, Obtained B.S. in Electronic Information School, Wuhan University, Wuhan, China.

<!-- # 💬 Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/) -->

# 💻 Internships
- *2020.12 - 2022.10*, YouTu Lab, Tencent, Shanghai, mentored by Researcher [Yabiao Wang](https://scholar.google.com.hk/citations?hl=zh-CN&user=xiK4nFUAAAAJ) and [Dr. Ying Tai](https://tyshiwo.github.io/).
- *2020.06 - 2020.10*, 2012 Lab, Huawei, Hangzhou.
- *2019.07 - 2019.09*, Guangzi Technique Center, Tencent, Shenzhen.
- *2019.04 - 2019.06*, Fuxi AI Lab, NetEase, Hangzhou.
